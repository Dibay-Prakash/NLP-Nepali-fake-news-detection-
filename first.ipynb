{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing dataset as fake and real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_fake \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfake.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m data_real \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data_fake = pd.read_csv('fake.csv')\n",
    "data_real = pd.read_csv('real.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clasify gareu data fake lai 0 le real lai 1 lw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake[\"class\"]= 0\n",
    "data_real[\"class\"]= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 tai data lai concatinate gareu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_real,data_fake],axis=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# droping the column named description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data= data.drop(['Description'],axis=1, inplace=False)\n",
    "cleaned_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# suffle gareu data lai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffle_data = cleaned_data.sample(frac=1)\n",
    "suffle_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding new index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to suffleing the index also gets suffeled.\n",
    "# So reseting new index of the dataframe to its default integer_based indexing.\n",
    "suffle_data.reset_index(inplace=True)\n",
    "suffle_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting the old index column\n",
    "suffle_data.drop(['index'],axis=1,inplace = True)\n",
    "suffle_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffle_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mixing both fake and real into 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffle_data.to_csv('mixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mixed.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordopt(Title):\n",
    "    \n",
    "    \n",
    "    # Title = Title.lower()\n",
    "    Title = re.sub(r'[\\[\\]]', '', Title)  # Remove square brackets\n",
    "    Title = re.sub(r'https?://\\S+|www\\.\\S+', '', Title)  # Remove URLs\n",
    "    Titel = re.sub(r'<.*?>', '', Title)  # Remove HTML tags\n",
    "    Title = re.sub(f'[{re.escape(string.punctuation)}]', '', Title)  # Remove punctuation\n",
    "    Title = re.sub(r'\\n', '', Title)  # Remove newlines\n",
    "    Title = re.sub(r'\\w*\\d\\w*', '', Title)  # Remove words with digits\n",
    "    Title = re.sub(r'\\b[a-zA-Z]+\\b', '', Title)  # This removes English words    \n",
    "    return Title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Title2']= data['Title'].apply (wordopt)\n",
    "data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "data['tokens1'] = data['Title2'].apply(word_tokenize)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# acessing the list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = open('stopwords.txt', encoding='utf-8')  # Specify encoding\n",
    "stopwords = []\n",
    "for x in stop: # perform through each line of the txt file ma.\n",
    "    x = x.replace('\\n', '')  # Remove newline characters\n",
    "    stopwords.append(x)      # Add to the stopwords list\n",
    "stop.close()  # Close the file after reading\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(x):\n",
    "    return ''.join([word for word in str(x).split() if word not in stopwords])\n",
    "data['title3']= data['tokens1']. apply(lambda x :remove_stopwords(x))# for each row in the tokens column \n",
    "#lambda function is called with x being the value of that row.\n",
    "#The result of the lambda function (which is the value returned by remove_stopwords(x)) is assigned to the new column title3.\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import snowballstemmer\n",
    "# test_words = [\"नेपालीको\", \"आफ्नै\", \"स्तेम्मेर\", \"यसलाई\", \"अनुमोदनको\"]\n",
    "#  stemmed_words = nepali_stemmer.stemWords(test_words)\n",
    "#  print(\"Original Words:\", test_words)\n",
    "#  print(\"Stemmed Words:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import snowballstemmer\n",
    "\n",
    "# # Initialize the NepaliStemmer instance\n",
    "# nepali_stemmer = snowballstemmer.NepaliStemmer()\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     if isinstance(text, str):\n",
    "#         # Remove punctuation and extra spaces\n",
    "#         text = re.sub(r'[^\\w\\s]', '', text)\n",
    "#         return text.strip()\n",
    "#     return text\n",
    "\n",
    "# def stem_text(text):\n",
    "#     text = preprocess_text(text)\n",
    "#     if isinstance(text, str):\n",
    "#         return \" \".join(nepali_stemmer.stemWords(text.split()))\n",
    "#     return text\n",
    "\n",
    "# # Apply the preprocessing and stemming to the 'title3' column\n",
    "# data['data4'] = data['title3'].apply(stem_text)\n",
    "\n",
    "# # Display results\n",
    "# print(data[['title3', 'data4']].head(20))\n",
    "# data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['title3']\n",
    "y = data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i built different file which contain only testing data\n",
    "x_test.to_csv('testing_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly building file for training data\n",
    "x_train.to_csv('training_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing the text data into numerical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  here due to the nepali datasets the system doest tfidf the data.\n",
    "# # so for that i have to convert the data into numerical featues using countvectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer(ngram_range=(1,2)).fit(x_train)\n",
    "# x_train_vectorized = vectorizer.transform(x_train)\n",
    "# print(x_train_vectorized)\n",
    "# x_train_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "# x_train_vectorized = vectorizer.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the text into vector The TfidfVectorizer is used to convert a collection of text documents into numerical vectors,\n",
    "#that can be fed into machine learning models. It does this by calculating the TF-IDF for each word in the documents.\n",
    "vectorization = TfidfVectorizer() \n",
    "xv_train = vectorization.fit_transform(x_train)\n",
    "xv_test = vectorization.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xv_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(xv_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr=model.predict(xv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(xv_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_label(n):\n",
    "    if n == 0:\n",
    "        return \"Fake News\"\n",
    "    elif n == 1:\n",
    "        return \"Real News\"\n",
    "\n",
    "def manual_testing(news):\n",
    "    # Create a DataFrame from the input news\n",
    "    testing_news = {\"title3\": [news]}\n",
    "    new_def_test = pd.DataFrame(testing_news)\n",
    "\n",
    "    # Apply preprocessing using wordopt (assuming it's a defined function)\n",
    "    new_def_test[\"title3\"] = new_def_test[\"title3\"].apply(wordopt) \n",
    "\n",
    "    # Transform the text using vectorization\n",
    "    new_x_test = new_def_test[\"title3\"]\n",
    "    new_xv_test = vectorization.transform(new_x_test)\n",
    "\n",
    "    # Perform prediction using Logistic Regression\n",
    "    pred_model = model.predict(new_xv_test)\n",
    "\n",
    "    # Output the prediction result\n",
    "    return print(\"\\n\\nLogistic Regression Prediction: {}\".format(output_label(pred_model[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_input = \"['उफ्','मिडियाले','झूट','बोल्योसैन्य','सदस्यहर...\"\n",
    "manual_testing(news_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# confusion metrics\n",
    "confusion_matrix(y_test, pred_lr,labels=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving the model using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an iterator object with write permission - model.pkl\n",
    "with open('model_pkl', 'wb') as files:\n",
    "    pickle.dump(model, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model\n",
    "with open('model_pkl' , 'rb') as file:\n",
    "    model= pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorized form ma lageko "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [\"उफ्','मिडियाले','झूट','बोल्योसैन्य','सदस्यहर..\"]\n",
    "input_vectorized = vectorization.transform(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict([[\"'राष्ट्रपति','पदका','उम्मेद्वार','मार्को','रु..\"]])\n",
    "prediction = model.predict(input_vectorized)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
